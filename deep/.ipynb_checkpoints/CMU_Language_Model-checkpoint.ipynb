{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling\n",
    "\n",
    "From the CMU course http://phontron.com/class/nn4nlp2017\n",
    "\n",
    "I guess the following several examples are from lecture 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 pierre <unk> N years old will join the board as a nonexecutive director nov. N\n",
      "2 mr. <unk> is chairman of <unk> n.v. the dutch publishing group\n",
      "3 rudolph <unk> N years old and former chairman of consolidated gold fields plc was named a nonexecutive director of this british industrial conglomerate\n",
      "4 a form of asbestos once used to make kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed to it more than N years ago researchers reported\n",
      "5 the asbestos fiber <unk> is unusually <unk> once it enters the <unk> with even brief exposures to it causing symptoms that show up decades later researchers said\n",
      "6 <unk> inc. the unit of new york-based <unk> corp. that makes kent cigarettes stopped using <unk> in its <unk> cigarette filters in N\n",
      "7 although preliminary findings were reported more than a year ago the latest results appear in today 's new england journal of medicine a forum likely to bring new attention to the problem\n",
      "8 a <unk> <unk> said this is an old story\n",
      "9 we 're talking about years ago before anyone heard of asbestos having any questionable properties\n",
      "10 there is no asbestos in our products now\n"
     ]
    }
   ],
   "source": [
    "# The length of the n-gram\n",
    "N = 2\n",
    "training_size = 10\n",
    "\n",
    "# Functions to read in the corpus\n",
    "# NOTE: We are using data from the Penn Treebank, which is already converted\n",
    "#       into an easy-to-use format with \"<unk>\" symbols. If we were using other\n",
    "#       data we would have to do pre-processing and consider how to choose\n",
    "#       unknown words, etc.\n",
    "device = torch.device(\"cpu\")\n",
    "w2i = defaultdict(lambda: torch.tensor(len(w2i), device=device))\n",
    "S = w2i[\"<s>\"]\n",
    "UNK = w2i[\"<unk>\"]\n",
    "def read_dataset(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            if i > training_size:\n",
    "                break\n",
    "            print i, line.strip()   \n",
    "            yield [w2i[x] for x in line.strip().split(\" \")]\n",
    "\n",
    "# Read in the data\n",
    "train = list(read_dataset(\"data/ptb/train.txt\"))\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "#dev = list(read_dataset(\"data/ptb/valid.txt\"))\n",
    "i2w = {v.item(): k for k, v in w2i.items()}\n",
    "nwords = len(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(2),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(7),\n",
       " tensor(8),\n",
       " tensor(9),\n",
       " tensor(10),\n",
       " tensor(11),\n",
       " tensor(12),\n",
       " tensor(13),\n",
       " tensor(14),\n",
       " tensor(3)]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pierre <unk> N years old will join the board as a nonexecutive director nov. N'"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([i2w[i.item()] for i in train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5668, -0.9921, -0.8663]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LogLin(torch.nn.Module):\n",
    "    #Simple additive model\n",
    "    def __init__(self, vocab_len):\n",
    "        super(LogLin, self).__init__()\n",
    "        \n",
    "        self.embed1 = torch.nn.Embedding(vocab_len, vocab_len)\n",
    "        self.embed2 = torch.nn.Embedding(vocab_len, vocab_len)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(1, vocab_len))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        word1, word2 = inputs\n",
    "        return F.log_softmax(self.embed1(word1)+self.embed2(word2)+self.bias, dim = 1)\n",
    "    \n",
    "model = LogLin(3)\n",
    "model([torch.tensor(1),torch.tensor(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1175.51071167\n",
      "1 1080.92725372\n",
      "2 1024.51812363\n",
      "3 968.134040833\n",
      "4 921.705257416\n",
      "5 866.729156494\n",
      "6 821.252960205\n",
      "7 773.63004303\n",
      "8 733.343540192\n",
      "9 692.92036438\n",
      "10 653.750497818\n",
      "11 623.500928879\n",
      "12 587.504966736\n",
      "13 553.747602463\n",
      "14 522.264720917\n",
      "15 492.827001572\n",
      "16 465.51442337\n",
      "17 440.890245438\n",
      "18 413.369297028\n",
      "19 385.621337891\n",
      "20 363.250916481\n",
      "21 340.145839691\n",
      "22 317.310652733\n",
      "23 299.64923954\n",
      "24 281.001944542\n",
      "25 267.551102638\n",
      "26 252.59250164\n",
      "27 237.609023094\n",
      "28 223.816936493\n",
      "29 210.883934021\n",
      "30 199.161718369\n",
      "31 190.241986275\n",
      "32 177.557794571\n",
      "33 169.100515842\n",
      "34 160.201855183\n",
      "35 153.005021095\n",
      "36 144.864591599\n",
      "37 139.812113285\n",
      "38 133.562878132\n",
      "39 128.6095047\n",
      "40 124.265469551\n",
      "41 120.325480461\n",
      "42 116.817925453\n",
      "43 112.718719006\n",
      "44 108.837460518\n",
      "45 105.59706068\n",
      "46 103.520357132\n",
      "47 100.644711494\n",
      "48 98.091196537\n",
      "49 95.7719306946\n",
      "50 93.5239610672\n",
      "51 92.0107827187\n",
      "52 90.2010707855\n",
      "53 88.9346356392\n",
      "54 87.0445551872\n",
      "55 85.9523730278\n",
      "56 84.9849567413\n",
      "57 83.5229434967\n",
      "58 82.3433227539\n",
      "59 80.9528694153\n",
      "60 80.0553479195\n",
      "61 78.8056697845\n",
      "62 77.7924442291\n",
      "63 76.5917410851\n",
      "64 76.3591637611\n",
      "65 74.7798905373\n",
      "66 74.1185665131\n",
      "67 73.1074666977\n",
      "68 72.5024528503\n",
      "69 71.8862400055\n",
      "70 70.5997929573\n",
      "71 70.4911999702\n",
      "72 69.8826189041\n",
      "73 68.4887609482\n",
      "74 68.0750589371\n",
      "75 67.2776007652\n",
      "76 67.0377511978\n",
      "77 65.9820513725\n",
      "78 66.3598124981\n",
      "79 65.3475549221\n",
      "80 64.6899728775\n",
      "81 64.3583090305\n",
      "82 64.0558772087\n",
      "83 63.6516382694\n",
      "84 63.5843892097\n",
      "85 62.7881338596\n",
      "86 62.619988203\n",
      "87 62.4048879147\n",
      "88 61.6471984386\n",
      "89 61.8022525311\n",
      "90 61.2477445602\n",
      "91 61.0883202553\n",
      "92 60.8195567131\n",
      "93 60.1475155354\n",
      "94 59.6109318733\n",
      "95 60.0086483955\n",
      "96 59.5021989346\n",
      "97 59.6929123402\n",
      "98 58.6922457218\n",
      "99 59.0160667896\n",
      "100 58.2438910007\n",
      "101 58.332710743\n",
      "102 57.307441473\n",
      "103 57.1572425365\n",
      "104 57.3020677567\n",
      "105 57.177977562\n",
      "106 56.6363434792\n",
      "107 56.8879656792\n",
      "108 56.343110323\n",
      "109 56.3671388626\n",
      "110 56.2408833504\n",
      "111 56.6087257862\n",
      "112 56.1641306877\n",
      "113 55.4998140335\n",
      "114 55.6081819534\n",
      "115 54.8187844753\n",
      "116 54.7788922787\n",
      "117 54.7422821522\n",
      "118 54.4355881214\n",
      "119 54.2535967827\n",
      "120 54.4534883499\n",
      "121 54.1912918091\n",
      "122 54.1271822453\n",
      "123 54.034727335\n",
      "124 53.6573910713\n",
      "125 53.9242677689\n",
      "126 53.073346138\n",
      "127 53.0303301811\n",
      "128 53.0849597454\n",
      "129 52.8863050938\n",
      "130 53.0673592091\n",
      "131 52.6321306229\n",
      "132 52.3271255493\n",
      "133 52.5054986477\n",
      "134 52.6980426311\n",
      "135 52.5952932835\n",
      "136 52.4916193485\n",
      "137 51.9747459888\n",
      "138 52.6876935959\n",
      "139 51.9485976696\n",
      "140 51.6616709232\n",
      "141 51.7739961147\n",
      "142 51.5653452873\n",
      "143 51.2841117382\n",
      "144 51.6485338211\n",
      "145 51.3534169197\n",
      "146 50.403642416\n",
      "147 50.4519579411\n",
      "148 51.0302336216\n",
      "149 50.9582047462\n",
      "150 50.5608451366\n",
      "151 51.0647685528\n",
      "152 50.7037463188\n",
      "153 50.5003304482\n",
      "154 50.7091569901\n",
      "155 50.3794174194\n",
      "156 50.2581810951\n",
      "157 49.607881546\n",
      "158 49.818980217\n",
      "159 49.6066946983\n",
      "160 49.9799127579\n",
      "161 49.5246918201\n",
      "162 49.6034588814\n",
      "163 50.1313779354\n",
      "164 49.9335918427\n",
      "165 49.8352103233\n",
      "166 49.624464035\n",
      "167 49.0661411285\n",
      "168 49.1561100483\n",
      "169 49.050983429\n",
      "170 49.147295475\n",
      "171 48.7920076847\n",
      "172 49.1503031254\n",
      "173 49.366756916\n",
      "174 48.9975688457\n",
      "175 48.4754068851\n",
      "176 48.6673545837\n",
      "177 48.3117837906\n",
      "178 48.5995788574\n",
      "179 49.147829771\n",
      "180 48.7264118195\n",
      "181 48.8720037937\n",
      "182 48.8280220032\n",
      "183 48.1177530289\n",
      "184 48.1410501003\n",
      "185 48.1390380859\n",
      "186 48.2755057812\n",
      "187 47.9005391598\n",
      "188 47.9954526424\n",
      "189 48.7741625309\n",
      "190 47.9462668896\n",
      "191 47.9311492443\n",
      "192 47.670678854\n",
      "193 47.8802957535\n",
      "194 47.9107506275\n",
      "195 47.8270378113\n",
      "196 47.6736083031\n",
      "197 47.536844492\n",
      "198 47.7055175304\n",
      "199 47.7137074471\n",
      "200 47.5535502434\n",
      "201 47.79946661\n",
      "202 47.0386500359\n",
      "203 47.1640942097\n",
      "204 47.042090416\n",
      "205 47.4677052498\n",
      "206 47.6811916828\n",
      "207 47.8045413494\n",
      "208 47.5023224354\n",
      "209 47.0212728977\n",
      "210 47.1827127934\n",
      "211 46.9182231426\n",
      "212 47.1935129166\n",
      "213 47.1249928474\n",
      "214 47.5240502357\n",
      "215 46.6686501503\n",
      "216 46.5938529968\n",
      "217 47.1123886108\n",
      "218 46.7775375843\n",
      "219 46.4784855843\n",
      "220 46.3000473976\n",
      "221 47.2892637253\n",
      "222 46.9386565685\n",
      "223 46.8460874557\n",
      "224 46.8906638622\n",
      "225 46.5881192684\n",
      "226 47.0230548382\n",
      "227 47.0416278839\n",
      "228 46.5056390762\n",
      "229 46.2466745377\n",
      "230 46.2245554924\n",
      "231 45.9844050407\n",
      "232 45.8000013828\n",
      "233 45.7681901455\n",
      "234 45.8933396339\n",
      "235 46.701385498\n",
      "236 46.4257750511\n",
      "237 46.0890402794\n",
      "238 45.9245657921\n",
      "239 45.7555787563\n",
      "240 46.4379019737\n",
      "241 46.1512639523\n",
      "242 45.94541502\n",
      "243 46.4594378471\n",
      "244 45.9885728359\n",
      "245 45.7792773247\n",
      "246 45.9068522453\n",
      "247 46.0107762814\n",
      "248 46.1132156849\n",
      "249 45.5187461376\n",
      "250 46.1965243816\n",
      "251 45.6455183029\n",
      "252 45.7526073456\n",
      "253 45.1800529957\n",
      "254 46.1563277245\n",
      "255 45.8429720402\n",
      "256 45.188893795\n",
      "257 45.1795668602\n",
      "258 45.2973799706\n",
      "259 45.7369225025\n",
      "260 45.8781321049\n",
      "261 45.5400235653\n",
      "262 44.9787786007\n",
      "263 45.0254712105\n",
      "264 45.5374984741\n",
      "265 45.6426436901\n",
      "266 45.4069612026\n",
      "267 45.2767696381\n",
      "268 45.3400337696\n",
      "269 44.9060900211\n",
      "270 45.1061241627\n",
      "271 45.9303166866\n",
      "272 45.460062027\n",
      "273 45.0532259941\n",
      "274 45.2168960571\n",
      "275 44.6924126148\n",
      "276 44.9671537876\n",
      "277 45.3083577156\n",
      "278 45.1827549934\n",
      "279 45.4100077152\n",
      "280 45.4952528477\n",
      "281 45.7751297951\n",
      "282 45.8046216965\n",
      "283 45.5556411743\n",
      "284 46.0040626526\n",
      "285 44.5386910439\n",
      "286 45.3488180637\n",
      "287 44.8025989532\n",
      "288 44.9891986847\n",
      "289 44.9451889992\n",
      "290 44.8027095795\n",
      "291 45.0170559883\n",
      "292 45.1122159958\n",
      "293 44.796620369\n",
      "294 44.5499315262\n",
      "295 45.393815279\n",
      "296 44.7892467976\n",
      "297 44.5034458637\n",
      "298 45.3084366322\n",
      "299 45.0526399612\n",
      "300 44.7273979187\n",
      "301 44.6043934822\n",
      "302 44.5385301113\n",
      "303 44.1322762966\n",
      "304 44.8817515373\n",
      "305 44.9077701569\n",
      "306 45.0381379128\n",
      "307 44.579770565\n",
      "308 44.7716748714\n",
      "309 44.451761961\n",
      "310 44.3472759724\n",
      "311 44.6843523979\n",
      "312 44.4658224583\n",
      "313 44.4626808167\n",
      "314 44.6781919003\n",
      "315 44.4243476391\n",
      "316 45.0922276974\n",
      "317 44.1479127407\n",
      "318 44.1843340397\n",
      "319 44.0663983822\n",
      "320 43.9234237671\n",
      "321 43.8454437256\n",
      "322 44.4722249508\n",
      "323 44.3122904301\n",
      "324 44.3253903389\n",
      "325 44.0539972782\n",
      "326 44.4358890057\n",
      "327 44.4103395939\n",
      "328 44.5507695675\n",
      "329 44.7444531918\n",
      "330 44.4750266075\n",
      "331 43.8618113995\n",
      "332 44.4551997185\n",
      "333 44.836294651\n",
      "334 43.7471687794\n",
      "335 43.6277990341\n",
      "336 43.913433075\n",
      "337 43.5692937374\n",
      "338 43.8809363842\n",
      "339 43.6615536213\n",
      "340 43.6835472584\n",
      "341 43.7262437344\n",
      "342 44.2314698696\n",
      "343 44.2189381123\n",
      "344 43.7628946304\n",
      "345 44.0734140873\n",
      "346 43.3194580078\n",
      "347 43.8857431412\n",
      "348 44.0825955868\n",
      "349 43.6816737652\n",
      "350 43.9550685883\n",
      "351 43.381519556\n",
      "352 43.9772224426\n",
      "353 44.2066566944\n",
      "354 43.5384061337\n",
      "355 44.0222377777\n",
      "356 44.0077693462\n",
      "357 43.4916360378\n",
      "358 43.6212348938\n",
      "359 43.7961704731\n",
      "360 44.3590698242\n",
      "361 44.7002658844\n",
      "362 44.5572674274\n",
      "363 45.0919251442\n",
      "364 45.1115086079\n",
      "365 44.3918921947\n",
      "366 44.531042099\n",
      "367 44.5582392216\n",
      "368 43.5066668987\n",
      "369 44.0110821724\n",
      "370 43.7647900581\n",
      "371 43.3774683475\n",
      "372 43.7414944172\n",
      "373 44.2107186317\n",
      "374 44.0657060146\n",
      "375 43.2287983894\n",
      "376 44.1235921383\n",
      "377 43.0463755131\n",
      "378 43.3952634335\n",
      "379 43.2326784134\n",
      "380 43.9246506691\n",
      "381 42.9399690628\n",
      "382 42.9528529644\n",
      "383 43.6987876892\n",
      "384 43.4706866741\n",
      "385 43.5307927132\n",
      "386 43.2042849064\n",
      "387 43.393848896\n",
      "388 44.3129456043\n",
      "389 43.9672319889\n",
      "390 43.555315733\n",
      "391 43.2031006813\n",
      "392 43.4315433502\n",
      "393 43.1045613289\n",
      "394 43.3464376926\n",
      "395 43.5774786472\n",
      "396 43.3123958111\n",
      "397 43.6893785\n",
      "398 43.0005104542\n",
      "399 43.5484166145\n"
     ]
    }
   ],
   "source": [
    "model = LogLin(nwords)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum = 0.5)\n",
    "\n",
    "short_train = train #[:10]\n",
    "for t in range(400):\n",
    "    model.zero_grad()\n",
    "    random.shuffle(short_train)\n",
    "    tot_loss = 0.0\n",
    "    for i, sentence in enumerate(short_train):\n",
    "        hist = [S, S]\n",
    "        losses = torch.tensor(0., device = device)\n",
    "        for next_word in sentence+[S]:\n",
    "            pred = model(hist)\n",
    "            losses += criterion(pred, torch.tensor([next_word], device = device))\n",
    "            hist = hist[1:] + [next_word]\n",
    "        losses.backward()\n",
    "        tot_loss += losses.item()\n",
    "        optimizer.step()\n",
    "    print t, tot_loss\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', \"'re\", 'talking', 'about', 'years', 'ago', 'before', 'anyone', 'heard', 'of', 'asbestos', 'having', 'any', 'questionable', 'properties']\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 100\n",
    "with torch.no_grad():\n",
    "    hist = [S] * N\n",
    "    sent = []\n",
    "    while True:\n",
    "        p = model(hist)\n",
    "        p = torch.exp(p[0]).numpy()\n",
    "        next_word = np.random.choice(nwords, p=p/p.sum())\n",
    "        # print next_word\n",
    "        if next_word == S or len(sent) == MAX_LEN:\n",
    "            break\n",
    "        sent.append(next_word)\n",
    "        hist = hist[1:] + [torch.tensor(next_word)]\n",
    "        \n",
    "    print [i2w[w] for w in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ukkk... This one is broken\n",
    "\n",
    "## Fixed it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7134, -1.6249, -1.1613]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NNLM(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_len, embedding_dim):\n",
    "        super(NNLM, self).__init__()\n",
    "        \n",
    "        self.embed = torch.nn.Embedding(vocab_len, embedding_dim)\n",
    "        self.linear = torch.nn.Linear(2*embedding_dim, vocab_len)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        word1, word2 = inputs\n",
    "        embedded = torch.cat((self.embed(word1), self.embed(word2))).view(1,-1)\n",
    "        # print embedded\n",
    "        return F.log_softmax(self.linear(torch.tanh(embedded)), dim = 1 )\n",
    "    \n",
    "model = NNLM(3, 5)\n",
    "model([torch.tensor(1),torch.tensor(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999 0.0198105722666\n",
      "1999 0.00922349840403\n",
      "2999 0.0059280670248\n",
      "3999 0.00434153573588\n",
      "4999 0.00341386138462\n",
      "5999 0.00280707702041\n",
      "6999 0.00238014617935\n",
      "7999 0.00206399708986\n",
      "8999 0.00182056985795\n",
      "9999 0.00162761716638\n"
     ]
    }
   ],
   "source": [
    "model = NNLM(2, 4)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "S = torch.tensor(0)\n",
    "\n",
    "short_train = [[torch.tensor(1)]]\n",
    "for t in range(10000):\n",
    "    \n",
    "    random.shuffle(short_train)\n",
    "    tot_loss = 0.0\n",
    "    for i, sentence in enumerate(short_train):\n",
    "        hist = [S, S]\n",
    "        losses = torch.tensor(0.)\n",
    "        optimizer.zero_grad()\n",
    "        for next_word in sentence[:1]+[S]:\n",
    "            \n",
    "            pred = model(hist)\n",
    "\n",
    "            losses += criterion(pred, next_word.view(1))\n",
    "            hist = hist[1:] + [next_word]\n",
    "        losses.backward()\n",
    "        tot_loss += losses.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if t%1000 == 999:\n",
    "        print t, tot_loss/len(short_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0), tensor(0)]\n",
      "tensor([[-7.1069, -0.0008]])\n",
      "[[8.193913e-04 9.991807e-01]]\n",
      "1\n",
      "[tensor(0), tensor(1)]\n",
      "tensor([[-0.0008, -7.1217]])\n",
      "[[9.9919254e-01 8.0740202e-04]]\n",
      "0\n",
      "breaking 1 0 tensor(0)\n",
      "['<s>']\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 10\n",
    "with torch.no_grad():\n",
    "    hist = [S] * 2\n",
    "    sent = []\n",
    "    while True:\n",
    "        print hist\n",
    "        p = model(hist)\n",
    "        print p\n",
    "        p = torch.exp(p).numpy()\n",
    "        print p\n",
    "        next_word = np.argmax(p)\n",
    "        print next_word\n",
    "        if next_word == S or len(sent) == MAX_LEN:\n",
    "            print \"breaking\", len(sent), next_word , S\n",
    "            break\n",
    "        sent.append(next_word)\n",
    "        hist = hist[1:] + [torch.tensor(next_word)]\n",
    "        \n",
    "    print [i2w[w] for w in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something is very broken... not sure who I can ask for help from though...\n",
    "\n",
    "Fixed it! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs\n",
    "\n",
    "And now lecture 6!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7678, -0.4086, -1.8036]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_len, embed_dim, hidden_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed = torch.nn.Embedding(vocab_len, embed_dim)\n",
    "        self.rnn = torch.nn.RNN(embed_dim, hidden_dim)\n",
    "        self.linear = torch.nn.Linear(hidden_dim, vocab_len)\n",
    "        self.hidden = None\n",
    "        self.initialize()\n",
    "    def forward(self, sentence):\n",
    "        embedding = self.embed(sentence).view(len(sentence),1,-1)\n",
    "        out, self.hidden = self.rnn(embedding, self.hidden)\n",
    "        return F.log_softmax(self.linear(out), dim = 2).view(1,-1)\n",
    "    \n",
    "    def initialize(self):\n",
    "        self.hidden = torch.zeros(1,1,self.hidden_dim)\n",
    "    \n",
    "rnn = RNN(3,16,2)\n",
    "rnn(torch.tensor([0]))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 10.4899823189\n",
      "199 5.1687895298\n",
      "299 3.99518003464\n",
      "399 3.52835900784\n",
      "499 3.28303370476\n",
      "599 3.13431785107\n",
      "699 2.98396658897\n",
      "799 2.86056520939\n",
      "899 2.7915494442\n",
      "999 2.74182777405\n"
     ]
    }
   ],
   "source": [
    "model = RNN(len(w2i),16,16)\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum = 0.1)\n",
    "\n",
    "\n",
    "train_short = train #[:500]\n",
    "for t in range(1000):\n",
    "    \n",
    "    random.shuffle(train_short)\n",
    "    train_loss = 0.0\n",
    "    for sentence in train_short:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model.initialize()\n",
    "\n",
    "        x = torch.tensor([w for w in sentence])\n",
    "        last_word = S\n",
    "        losses = torch.tensor(0.)\n",
    "        for word in sentence + [S]:\n",
    "            y_pred = model(last_word.view(1))\n",
    "            losses += criterion(y_pred, torch.tensor([word]))\n",
    "            last_word = word\n",
    "        train_loss += losses.item()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "    if t%100 == 99:\n",
    "        print t, train_loss/len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breaking 34 0 tensor(0)\n",
      "['a', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among', 'a', 'group', 'of', 'workers', 'exposed', 'to', 'it', 'more', 'than', 'N', 'years', 'ago', 'researchers', 'reported']\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 100\n",
    "with torch.no_grad():\n",
    "    model.initialize()\n",
    "    hist = S\n",
    "    sent = []\n",
    "    while True:\n",
    "        p = model(hist.view(1))\n",
    "        p = torch.exp(p).numpy()[0]\n",
    "\n",
    "        next_word = np.random.choice([torch.tensor(i) for i in range(len(w2i))], p = p/p.sum())\n",
    "        if next_word == S or len(sent) == MAX_LEN:\n",
    "            print \"breaking\", len(sent), next_word , S\n",
    "            break\n",
    "        sent.append(next_word)\n",
    "        hist = torch.tensor(next_word)\n",
    "        \n",
    "    print [i2w[w] for w in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 pierre <unk> N years old will join the board as a nonexecutive director nov. N\n",
      "2 mr. <unk> is chairman of <unk> n.v. the dutch publishing group\n",
      "3 rudolph <unk> N years old and former chairman of consolidated gold fields plc was named a nonexecutive director of this british industrial conglomerate\n",
      "4 a form of asbestos once used to make kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed to it more than N years ago researchers reported\n",
      "5 the asbestos fiber <unk> is unusually <unk> once it enters the <unk> with even brief exposures to it causing symptoms that show up decades later researchers said\n",
      "6 <unk> inc. the unit of new york-based <unk> corp. that makes kent cigarettes stopped using <unk> in its <unk> cigarette filters in N\n",
      "7 although preliminary findings were reported more than a year ago the latest results appear in today 's new england journal of medicine a forum likely to bring new attention to the problem\n",
      "8 a <unk> <unk> said this is an old story\n",
      "9 we 're talking about years ago before anyone heard of asbestos having any questionable properties\n",
      "10 there is no asbestos in our products now\n",
      "[3, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[16, 2, 17, 18, 19, 2, 20, 9, 21, 22, 23, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[24, 2, 4, 5, 6, 25, 26, 18, 19, 27, 28, 29, 30, 31, 32, 12, 13, 14, 19, 33, 34, 35, 36, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[12, 37, 19, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 12, 48, 49, 19, 50, 51, 52, 12, 23, 19, 53, 54, 41, 55, 56, 57, 4, 5, 58, 59, 60, 1]\n",
      "[9, 38, 61, 2, 17, 62, 2, 39, 55, 63, 9, 2, 64, 65, 66, 67, 41, 55, 68, 69, 70, 71, 72, 73, 74, 59, 75, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2, 76, 9, 77, 19, 78, 79, 2, 80, 70, 81, 43, 82, 83, 84, 2, 85, 86, 2, 44, 45, 85, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[87, 88, 89, 90, 60, 56, 57, 12, 91, 58, 9, 92, 93, 94, 85, 95, 96, 78, 97, 98, 19, 99, 12, 100, 101, 41, 102, 78, 103, 41, 9, 104, 1, 0, 0]\n",
      "[12, 2, 2, 75, 33, 17, 105, 6, 106, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[107, 108, 109, 110, 5, 58, 111, 112, 113, 19, 38, 114, 115, 116, 117, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[118, 17, 119, 38, 85, 120, 121, 122, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "training_size = 10\n",
    "\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "PAD = w2i[\"<p>\"]\n",
    "S = w2i[\"<s>\"]\n",
    "UNK = w2i[\"<unk>\"]\n",
    "MAX_SENTENCE_LENGTH = -1\n",
    "\n",
    "def read_dataset(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            if i > training_size:\n",
    "                break\n",
    "            print i, line.strip() \n",
    "            sentence = line.strip().split(\" \") + [\"<s>\"]\n",
    "            global MAX_SENTENCE_LENGTH\n",
    "            if len(sentence) > MAX_SENTENCE_LENGTH:\n",
    "                MAX_SENTENCE_LENGTH = len(sentence)\n",
    "            yield [w2i[x] for x in sentence]\n",
    "\n",
    "# Read in the data\n",
    "train = list(read_dataset(\"data/ptb/train.txt\"))\n",
    "for t in train:\n",
    "    while len(t) < MAX_SENTENCE_LENGTH:\n",
    "        t.append(0)\n",
    "    print t\n",
    "            \n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "\n",
    "i2w = {v: k for k, v in w2i.items()}\n",
    "nwords = len(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4656, -1.7887, -1.5845],\n",
       "         [-0.5929, -1.5279, -1.4686]],\n",
       "\n",
       "        [[-0.5999, -1.4814, -1.4970],\n",
       "         [-0.4917, -1.7055, -1.5762]],\n",
       "\n",
       "        [[-0.6117, -1.4467, -1.5039],\n",
       "         [-0.6115, -1.4541, -1.4969]],\n",
       "\n",
       "        [[-0.5289, -1.6088, -1.5577],\n",
       "         [-0.5232, -1.6227, -1.5605]],\n",
       "\n",
       "        [[-0.6195, -1.4344, -1.4981],\n",
       "         [-0.6419, -1.4143, -1.4669]]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BatchRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_len, batch_size, sentence_length, embed_dim, hidden_dim):\n",
    "        super(BatchRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.sentence_length = sentence_length\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embed = torch.nn.Embedding(vocab_len, embed_dim, padding_idx=0)\n",
    "        self.rnn = torch.nn.RNN(embed_dim, hidden_dim)\n",
    "        self.linear = torch.nn.Linear(hidden_dim, vocab_len)\n",
    "        self.hidden = None\n",
    "        self.initialize()\n",
    "    def forward(self, sentences):\n",
    "        embedding = self.embed(sentences.t()).view(self.sentence_length,self.batch_size,self.embed_dim)\n",
    "        out, self.hidden = self.rnn(embedding, self.hidden)\n",
    "        return F.log_softmax(self.linear(out), dim = 2)\n",
    "    \n",
    "    def initialize(self):\n",
    "        self.hidden = torch.zeros(1,self.batch_size, self.hidden_dim)\n",
    "    \n",
    "rnn = BatchRNN(vocab_len = 3, batch_size = 2, sentence_length = 5, embed_dim = 3, hidden_dim = 2)\n",
    "rnn(torch.tensor([[1,0,0,1,0], [2,1,0,1,2]]))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "99 0.056821000576\n",
      "199 0.0209852322936\n",
      "299 0.0152653649449\n",
      "399 0.0136441558599\n",
      "499 0.0163279771805\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "\n",
    "model = BatchRNN(vocab_len = len(w2i), batch_size = batch_size, \n",
    "                sentence_length = MAX_SENTENCE_LENGTH, embed_dim = 16, hidden_dim = 16)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "train_short = train[:batch_size]\n",
    "print len(train_short) #[:500]\n",
    "for t in range(500):\n",
    "     \n",
    "    optimizer.zero_grad()\n",
    "    model.initialize()\n",
    "\n",
    "    y_pred = model(torch.tensor(train_short))\n",
    "\n",
    "    loss = loss_fn(y_pred.view(35, -1, batch_size), torch.tensor(train_short).t())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if t%100 == 99:\n",
    "        print t, loss.item()/len(train_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
