{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW and CBOW Sentiment\n",
    "\n",
    "Not totally sure which of these are really BOW and CBOW or some mixture between the two...\n",
    "\n",
    "Also, this is from the [CMU](http://phontron.com/class/nn4nlp2017/index.html) class (if that wasn't apparent from the name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to read in the corpus\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "t2i = defaultdict(lambda: len(t2i))\n",
    "UNK = w2i[\"<unk>\"]\n",
    "def read_dataset(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            tag, words = line.lower().strip().split(\" ||| \")\n",
    "            yield ([w2i[x] for x in words.split(\" \")], t2i[tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "train = list(read_dataset(\"data/classes/train.txt\"))\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "dev = list(read_dataset(\"data/classes/test.txt\"))\n",
    "nwords = len(w2i)\n",
    "ntags = len(t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"3 ||| The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents = open(\"data/classes/train.txt\").readlines()\n",
    "print len(train_sents)\n",
    "train_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CBOW(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CBOW, self).__init__()       \n",
    "        self.embeddings = torch.nn.Embedding(len(w2i), len(t2i))\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(len(t2i)).view(1,-1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        return F.log_softmax(embeds.sum(dim=0).view(1,-1) + self.bias, dim=1)\n",
    "    \n",
    "class CBOW2(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(CBOW2, self).__init__()       \n",
    "        self.embeddings = torch.nn.Embedding(len(w2i), hidden_dim)\n",
    "        self.linear = torch.nn.Linear(hidden_dim, len(t2i))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).sum(dim=0).view(1,-1)\n",
    "        return F.log_softmax(self.linear(embeds), dim=1)\n",
    "    \n",
    "class CBOW3(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(CBOW3, self).__init__()       \n",
    "        self.embeddings = torch.nn.Embedding(len(w2i), hidden_dim)\n",
    "        self.linear = torch.nn.Linear(hidden_dim, len(t2i))\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).sum(dim=0).view(1,-1)\n",
    "        out = self.dropout(self.linear(embeds))\n",
    "        return F.log_softmax(out, dim=1)\n",
    "    \n",
    "class BOW(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(BOW, self).__init__()       \n",
    "        self.embeddings = torch.nn.Embedding(len(w2i), len(t2i))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        return F.log_softmax(embeds, dim=1).sum(dim=0).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.74153947093\n",
      "1 2.45141218083\n",
      "2 1.83268226158\n",
      "3 1.41387938882\n",
      "4 1.130836651\n",
      "5 0.919660911346\n",
      "6 0.776208007195\n",
      "7 0.655590380669\n",
      "8 0.56924832905\n",
      "9 0.493897508459\n",
      "10 0.437929901779\n",
      "11 0.388300467004\n",
      "12 0.354219600436\n",
      "13 0.32413468585\n",
      "14 0.296508759719\n",
      "15 0.275947284637\n",
      "16 0.256219537897\n",
      "17 0.239081522487\n",
      "18 0.225710657498\n",
      "19 0.211817980222\n",
      "20 0.202557847719\n",
      "21 0.19092202372\n",
      "22 0.182301518212\n",
      "23 0.172945399923\n",
      "24 0.168296459858\n",
      "25 0.160233159597\n",
      "26 0.153083664796\n",
      "27 0.14817414706\n",
      "28 0.143048783787\n",
      "29 0.137557932096\n",
      "30 0.133083568824\n",
      "31 0.129573064616\n",
      "32 0.125685564356\n",
      "33 0.121678461507\n",
      "34 0.117801034805\n",
      "35 0.115255029134\n",
      "36 0.112374842289\n",
      "37 0.10900549379\n",
      "38 0.106461809872\n",
      "39 0.104057779038\n"
     ]
    }
   ],
   "source": [
    "model = CBOW()\n",
    "model.train()\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "for t in range(40):\n",
    "    \n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    for words, tag in train:\n",
    "        optimizer.zero_grad()\n",
    "        x = torch.tensor([w for w in words])\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = criterion(y_pred, torch.tensor([tag]))\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print t, train_loss/len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev 0.328959276018\n",
      "Train 0.992041198502\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    for words, tag in dev:\n",
    "        x = torch.tensor([w for w in words])\n",
    "        y_pred = model(x)\n",
    "        correct += torch.argmax(y_pred, dim = 1).item() == tag\n",
    "\n",
    "    print \"dev\", float(correct)/len(dev)\n",
    "    correct = 0\n",
    "    for words, tag in train:\n",
    "        x = torch.tensor([w for w in words])\n",
    "        y_pred = model(x)\n",
    "        correct += torch.argmax(y_pred, dim = 1).item() == tag\n",
    "\n",
    "    print \"Train\", float(correct)/len(train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.98368786606\n"
     ]
    }
   ],
   "source": [
    "# model = CBOW3(50)\n",
    "# model.train()\n",
    "# criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "# for t in range(1):\n",
    "    \n",
    "#     random.shuffle(train)\n",
    "#     train_loss = 0.0\n",
    "#     for words, tag in train:\n",
    "#         optimizer.zero_grad()\n",
    "#         x = torch.tensor([w for w in words])\n",
    "#         y_pred = model(x)\n",
    "\n",
    "#         loss = criterion(y_pred, torch.tensor([tag]))\n",
    "#         train_loss += loss.item()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print t, train_loss/len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev 0.354751131222\n",
      "Train 0.949672284644\n"
     ]
    }
   ],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     for words, tag in dev:\n",
    "#         x = torch.tensor([w for w in words])\n",
    "#         y_pred = model(x)\n",
    "#         correct += torch.argmax(y_pred, dim = 1).item() == tag\n",
    "\n",
    "#     print \"dev\", float(correct)/len(dev)\n",
    "#     correct = 0\n",
    "#     for words, tag in train:\n",
    "#         x = torch.tensor([w for w in words])\n",
    "#         y_pred = model(x)\n",
    "#         correct += torch.argmax(y_pred, dim = 1).item() == tag\n",
    "\n",
    "#     print \"Train\", float(correct)/len(train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sentiment(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sentiment, self).__init__()       \n",
    "        self.embeddings = torch.nn.Embedding(len(w2i), 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        # print \"embeds\", embeds.sum(dim=0)\n",
    "        return torch.sigmoid(embeds.sum(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.41802108698\n",
      "1 1.33709841283\n",
      "2 0.835866116547\n",
      "3 0.586966595206\n",
      "4 0.435553118778\n",
      "5 0.33201093033\n",
      "6 0.250492178666\n",
      "7 0.199820565287\n",
      "8 0.149695967955\n",
      "9 0.118060995762\n",
      "10 0.0773955546789\n",
      "11 0.0685972794444\n",
      "12 0.0529615695594\n",
      "13 0.0337114673106\n",
      "14 0.0321699700949\n",
      "15 0.0310849630382\n",
      "16 0.0286231650666\n",
      "17 0.0267585572528\n",
      "18 0.0245588197573\n",
      "19 0.0224945617789\n",
      "20 0.0222473219775\n",
      "21 0.0217625251784\n",
      "22 0.0217446602817\n",
      "23 0.0215785687158\n",
      "24 0.0214353888665\n",
      "25 0.021317885629\n",
      "26 0.0213136909239\n",
      "27 0.0212321412386\n",
      "28 0.0212236477079\n",
      "29 0.0211467029458\n",
      "30 0.0210595684644\n",
      "31 0.02102685447\n",
      "32 0.020967209591\n",
      "33 0.0209652713155\n",
      "34 0.0209372561681\n",
      "35 0.0208828903269\n",
      "36 0.0208587476873\n",
      "37 0.0208273177294\n",
      "38 0.0208158793943\n",
      "39 0.0207795743961\n"
     ]
    }
   ],
   "source": [
    "model = Sentiment()\n",
    "criterion = torch.nn.BCELoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-0)\n",
    "\n",
    "for t in range(40):\n",
    "    \n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    for words, tag in train:\n",
    "        if tag == 2:\n",
    "            continue\n",
    "        if tag < 2:\n",
    "            tag = 0.0\n",
    "        if tag > 2:\n",
    "            tag = 1.0\n",
    "        optimizer.zero_grad()\n",
    "        x = torch.tensor([w for w in words])\n",
    "        y_pred = model(x)\n",
    "        # print y_pred\n",
    "        loss = criterion(y_pred, torch.tensor([tag]))\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print t, train_loss/len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev 0.783635365184\n",
      "Train 0.999132947977\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    using = 0\n",
    "    for words, tag in dev:\n",
    "        if tag == 2:\n",
    "            continue\n",
    "        using += 1\n",
    "        if tag < 2:\n",
    "            tag = 0.0\n",
    "        if tag > 2:\n",
    "            tag = 1.0\n",
    "        x = torch.tensor([w for w in words])\n",
    "        y_pred = model(x)\n",
    "        correct += abs(tag - y_pred.item()) < 0.5\n",
    "\n",
    "    print \"dev\", float(correct)/using\n",
    "    correct = 0\n",
    "    using = 0\n",
    "    for words, tag in train:\n",
    "        if tag == 2:\n",
    "            continue\n",
    "        using += 1\n",
    "        if tag < 2:\n",
    "            tag = 0.0\n",
    "        if tag > 2:\n",
    "            tag = 1.0\n",
    "        x = torch.tensor([w for w in words])\n",
    "        y_pred = model(x)\n",
    "        # print y_pred\n",
    "        correct += abs(tag - y_pred.item()) < 0.5\n",
    "\n",
    "    print \"Train\", float(correct)/using  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4653], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, window_size, embedding_dim, output_channels):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        assert window_size%2 == 1 #odd window sizes please\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_channels = output_channels\n",
    "        \n",
    "        self.embed = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.cnn = torch.nn.Conv1d(embedding_dim, output_channels, window_size, padding = (window_size - 1)/2) \n",
    "        self.linear = torch.nn.Linear(output_channels, 1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embed(inputs).t().view(1,self.embedding_dim, -1)\n",
    "        convoluted = self.cnn(embedded)\n",
    "        pooled = F.relu(F.max_pool1d(convoluted, len(inputs)))\n",
    "        return torch.sigmoid(self.linear(pooled.view(self.output_channels)))\n",
    "    \n",
    "cnn = CNN(5,3,32,16)\n",
    "input = torch.tensor([0,1,2,1,4]) \n",
    "# print input\n",
    "cnn(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.559116127963\n",
      "1 0.546992398338\n",
      "2 0.537920873117\n",
      "3 0.529935765009\n",
      "4 0.522986482761\n",
      "5 0.514049974196\n",
      "6 0.506678677761\n",
      "7 0.496691504355\n",
      "8 0.484541071418\n",
      "9 0.478142225802\n",
      "10 0.47074982075\n",
      "11 0.458209036435\n",
      "12 0.449260221628\n",
      "13 0.438598727762\n",
      "14 0.427408823783\n",
      "15 0.418008087933\n",
      "16 0.409858000395\n",
      "17 0.396625886976\n",
      "18 0.38380466141\n",
      "19 0.371819286575\n",
      "20 0.357757684014\n",
      "21 0.34230660512\n",
      "22 0.333373975069\n",
      "23 0.312854016985\n",
      "24 0.296530620671\n",
      "25 0.282677547839\n",
      "26 0.266663719459\n",
      "27 0.242970353538\n",
      "28 0.228311647407\n",
      "29 0.211016473039\n",
      "30 0.195309464113\n",
      "31 0.1568078539\n",
      "32 0.141335186566\n",
      "33 0.12904788398\n",
      "34 0.107520385369\n",
      "35 0.0945435691704\n",
      "36 0.0705132258136\n",
      "37 0.0588438163109\n",
      "38 0.0405577773309\n",
      "39 0.026601341458\n"
     ]
    }
   ],
   "source": [
    "model = CNN(len(w2i),3,16,16)\n",
    "\n",
    "criterion = torch.nn.BCELoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum = 0.1)\n",
    "\n",
    "for t in range(40):\n",
    "    \n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    for words, tag in train:\n",
    "        if tag == 2:\n",
    "            continue\n",
    "        if tag < 2:\n",
    "            tag = 0.0\n",
    "        if tag > 2:\n",
    "            tag = 1.0\n",
    "        optimizer.zero_grad()\n",
    "        x = torch.tensor([w for w in words])\n",
    "        y_pred = model(x)\n",
    "        # print y_pred\n",
    "        loss = criterion(y_pred, torch.tensor([tag]))\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print t, train_loss/len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev 0.668863261944\n",
      "Train 0.996242774566\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    using = 0\n",
    "    for words, tag in dev:\n",
    "        if tag == 2:\n",
    "            continue\n",
    "        using += 1\n",
    "        if tag < 2:\n",
    "            tag = 0.0\n",
    "        if tag > 2:\n",
    "            tag = 1.0\n",
    "        x = torch.tensor([w for w in words])\n",
    "        y_pred = model(x)\n",
    "        correct += abs(tag - y_pred.item()) < 0.5\n",
    "\n",
    "    print \"dev\", float(correct)/using\n",
    "    correct = 0\n",
    "    using = 0\n",
    "    for words, tag in train:\n",
    "        if tag == 2:\n",
    "            continue\n",
    "        using += 1\n",
    "        if tag < 2:\n",
    "            tag = 0.0\n",
    "        if tag > 2:\n",
    "            tag = 1.0\n",
    "        x = torch.tensor([w for w in words])\n",
    "        y_pred = model(x)\n",
    "        # print y_pred\n",
    "        correct += abs(tag - y_pred.item()) < 0.5\n",
    "\n",
    "    print \"Train\", float(correct)/using  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
