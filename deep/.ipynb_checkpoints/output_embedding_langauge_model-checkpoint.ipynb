{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distantly tring to follow along on this https://arxiv.org/abs/1608.05859"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5289, -1.8399, -1.4751, -1.8754, -1.4174], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.input_embedding = nn.Linear(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.output_embedding =  nn.Linear(embedding_dim, vocab_size)\n",
    "        self.input_embedding.weight.data = self.output_embedding.weight.data.transpose(0,1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = torch.zeros(self.embedding_dim)\n",
    "        for input in inputs:\n",
    "            embeds += self.input_embedding.weight.data[:,input]\n",
    "        drop = torch.nn.Dropout(0.5)\n",
    "        out = torch.tanh(drop(self.linear(embeds)))\n",
    "        out = self.output_embedding(out)\n",
    "\n",
    "        log_probs = F.log_softmax(out, dim=0)\n",
    "        return log_probs\n",
    "\n",
    "raw_text = \"\"\"I like cats\n",
    "I like dogs\n",
    "we like cats\n",
    "we like dogs\"\"\".lower().split(\"\\n\") \n",
    "#print raw_text\n",
    "\n",
    "data = []\n",
    "vocab = []\n",
    "for sentence in raw_text:\n",
    "    words = sentence.split()\n",
    "    for i, word in enumerate(words):\n",
    "        vocab.append(word)\n",
    "        if i == 0:\n",
    "            context = [words[i+1], words[i+2]]\n",
    "        elif i == len(words) - 1:\n",
    "            context = [words[i-1], words[i-2]]\n",
    "        else:\n",
    "            context = [words[i-1], words[i+1]]\n",
    "        data.append((context, word))\n",
    "        \n",
    "vocab = set(vocab)\n",
    "\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "# print data\n",
    "ngram = NGramLanguageModeler(vocab_size, 10)\n",
    "ngram(torch.tensor([1], dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.82182794109\n",
      "0.616729064476\n",
      "0.603718156415\n",
      "0.578988845563\n",
      "0.568767220169\n",
      "0.579611914837\n",
      "0.570812347806\n",
      "0.560428542209\n",
      "0.56582658869\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "loss_fn = nn.NLLLoss()\n",
    "ngram = NGramLanguageModeler(vocab_size, 6)\n",
    "optimizer = torch.optim.Adam(ngram.parameters(), lr=1e-3)\n",
    "total_losses = 0\n",
    "step_size = 10000\n",
    "for t in range(100000):\n",
    "    ngram.zero_grad()\n",
    "    r_c = random.choice(data)\n",
    "\n",
    "    c_v = make_context_vector(r_c[0], word_to_ix)\n",
    "    output = ngram(c_v)\n",
    "\n",
    "    loss = loss_fn(output.view(1,-1), torch.tensor([word_to_ix[r_c[1]]]))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if t%step_size == 0:\n",
    "        print total_losses/step_size\n",
    "        total_losses = 0\n",
    "    total_losses += loss.item()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.2014, -0.5674,  0.2388,  0.5209,  1.8421, -2.1559])\n",
      "i tensor(1.)\n",
      "we tensor(0.9994)\n",
      "cats tensor(-0.8740)\n",
      "like tensor(-0.2435)\n",
      "dogs tensor(-0.8699)\n"
     ]
    }
   ],
   "source": [
    "ngram.eval()\n",
    "with torch.no_grad():\n",
    "    process = torch.tensor([word_to_ix[\"i\"]])\n",
    "    process_embed = ngram.input_embedding.weight.data[:,process].t()[0]\n",
    "    print process_embed\n",
    "    cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "    for word in word_to_ix:\n",
    "        other_embed = ngram.input_embedding.weight.data[:,torch.tensor([word_to_ix[word]])].t()[0]\n",
    "        print word, cos(process_embed, other_embed) #.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
