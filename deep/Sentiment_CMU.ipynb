{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW and CBOW Sentiment\n",
    "\n",
    "Not totally sure which of these are really BOW and CBOW or some mixture between the two...\n",
    "\n",
    "Also, this is from the [CMU](http://phontron.com/class/nn4nlp2017/index.html) class (if that wasn't apparent from the name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to read in the corpus\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "t2i = defaultdict(lambda: len(t2i))\n",
    "UNK = w2i[\"<unk>\"]\n",
    "def read_dataset(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            tag, words = line.lower().strip().split(\" ||| \")\n",
    "            yield ([w2i[x] for x in words.split(\" \")], t2i[tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "train = list(read_dataset(\"data/classes/train.txt\"))\n",
    "random.shuffle(train)\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "dev = list(read_dataset(\"data/classes/test.txt\"))\n",
    "nwords = len(w2i)\n",
    "ntags = len(t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"3 ||| The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents = open(\"data/classes/train.txt\").readlines()\n",
    "print (len(train_sents))\n",
    "train_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CBOW(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CBOW, self).__init__()       \n",
    "        self.embeddings = torch.nn.Embedding(len(w2i), len(t2i))\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(len(t2i)).view(1,-1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        return F.log_softmax(embeds.sum(dim=0).view(1,-1) + self.bias, dim=1)\n",
    "    \n",
    "class CBOW2(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(CBOW2, self).__init__()       \n",
    "        self.embeddings = torch.nn.Embedding(len(w2i), hidden_dim)\n",
    "        self.linear = torch.nn.Linear(hidden_dim, len(t2i))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).sum(dim=0).view(1,-1)\n",
    "        return F.log_softmax(self.linear(embeds), dim=1)\n",
    "    \n",
    "class CBOW3(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(CBOW3, self).__init__()       \n",
    "        self.embeddings = torch.nn.Embedding(len(w2i), hidden_dim)\n",
    "        self.linear = torch.nn.Linear(hidden_dim, len(t2i))\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).sum(dim=0).view(1,-1)\n",
    "        out = self.dropout(self.linear(embeds))\n",
    "        return F.log_softmax(out, dim=1)\n",
    "    \n",
    "class BOW(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(BOW, self).__init__()       \n",
    "        self.embeddings = torch.nn.Embedding(len(w2i), len(t2i))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        return F.log_softmax(embeds, dim=1).sum(dim=0).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.709558172132933\n",
      "1 2.465143023814163\n",
      "2 1.834459099004429\n",
      "3 1.435342233773243\n",
      "4 1.144472440846312\n",
      "5 0.9405416068535692\n",
      "6 0.7848543223704214\n",
      "7 0.6594357753147315\n",
      "8 0.5684988972053117\n",
      "9 0.4946089000837584\n",
      "10 0.4470698067193624\n",
      "11 0.3983736683385701\n",
      "12 0.3577868537908604\n",
      "13 0.32941330506672245\n",
      "14 0.29942043866076046\n",
      "15 0.27762210196943515\n",
      "16 0.2591409451355754\n",
      "17 0.24231784828653072\n",
      "18 0.2278954951667317\n",
      "19 0.2145715494312648\n",
      "20 0.20294356728154692\n",
      "21 0.19202283009782165\n",
      "22 0.18433360276220828\n",
      "23 0.1751796147665375\n",
      "24 0.16770824875247936\n",
      "25 0.16079857511569265\n",
      "26 0.15473037727163283\n",
      "27 0.1480980140488112\n",
      "28 0.1435162738673898\n",
      "29 0.13818768860316188\n",
      "30 0.13440050533678682\n",
      "31 0.1294369723726822\n",
      "32 0.12572743762643343\n",
      "33 0.1221397802162686\n",
      "34 0.11838425910560799\n",
      "35 0.11487677544010727\n",
      "36 0.11179136968183917\n",
      "37 0.10863384081444626\n",
      "38 0.10631993477358817\n",
      "39 0.10367241447304806\n"
     ]
    }
   ],
   "source": [
    "model = CBOW()\n",
    "model.train()\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "for t in range(40):\n",
    "    \n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    for words, tag in train:\n",
    "        optimizer.zero_grad()\n",
    "        x = torch.tensor([w for w in words])\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = criterion(y_pred, torch.tensor([tag]))\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print (t, train_loss/len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev 0.36787330316742084\n",
      "Train 0.9921582397003745\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    for words, tag in dev:\n",
    "        x = torch.tensor([w for w in words])\n",
    "        y_pred = model(x)\n",
    "        correct += torch.argmax(y_pred, dim = 1).item() == tag\n",
    "\n",
    "    print (\"dev\", float(correct)/len(dev))\n",
    "    correct = 0\n",
    "    for words, tag in train:\n",
    "        x = torch.tensor([w for w in words])\n",
    "        y_pred = model(x)\n",
    "        correct += torch.argmax(y_pred, dim = 1).item() == tag\n",
    "\n",
    "    print (\"Train\", float(correct)/len(train))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.98368786606\n"
     ]
    }
   ],
   "source": [
    "# model = CBOW3(50)\n",
    "# model.train()\n",
    "# criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "# for t in range(1):\n",
    "    \n",
    "#     random.shuffle(train)\n",
    "#     train_loss = 0.0\n",
    "#     for words, tag in train:\n",
    "#         optimizer.zero_grad()\n",
    "#         x = torch.tensor([w for w in words])\n",
    "#         y_pred = model(x)\n",
    "\n",
    "#         loss = criterion(y_pred, torch.tensor([tag]))\n",
    "#         train_loss += loss.item()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print (t, train_loss/len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev 0.354751131222\n",
      "Train 0.949672284644\n"
     ]
    }
   ],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     for words, tag in dev:\n",
    "#         x = torch.tensor([w for w in words])\n",
    "#         y_pred = model(x)\n",
    "#         correct += torch.argmax(y_pred, dim = 1).item() == tag\n",
    "\n",
    "#     print (\"dev\", float(correct)/len(dev))\n",
    "#     correct = 0\n",
    "#     for words, tag in train:\n",
    "#         x = torch.tensor([w for w in words])\n",
    "#         y_pred = model(x)\n",
    "#         correct += torch.argmax(y_pred, dim = 1).item() == tag\n",
    "\n",
    "#     print (\"Train\", float(correct)/len(train)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sentiment(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sentiment, self).__init__()       \n",
    "        self.embeddings = torch.nn.Embedding(len(w2i), 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        # print \"embeds\", embeds.sum(dim=0)\n",
    "        return torch.sigmoid(embeds.sum(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.41802108698\n",
      "1 1.33709841283\n",
      "2 0.835866116547\n",
      "3 0.586966595206\n",
      "4 0.435553118778\n",
      "5 0.33201093033\n",
      "6 0.250492178666\n",
      "7 0.199820565287\n",
      "8 0.149695967955\n",
      "9 0.118060995762\n",
      "10 0.0773955546789\n",
      "11 0.0685972794444\n",
      "12 0.0529615695594\n",
      "13 0.0337114673106\n",
      "14 0.0321699700949\n",
      "15 0.0310849630382\n",
      "16 0.0286231650666\n",
      "17 0.0267585572528\n",
      "18 0.0245588197573\n",
      "19 0.0224945617789\n",
      "20 0.0222473219775\n",
      "21 0.0217625251784\n",
      "22 0.0217446602817\n",
      "23 0.0215785687158\n",
      "24 0.0214353888665\n",
      "25 0.021317885629\n",
      "26 0.0213136909239\n",
      "27 0.0212321412386\n",
      "28 0.0212236477079\n",
      "29 0.0211467029458\n",
      "30 0.0210595684644\n",
      "31 0.02102685447\n",
      "32 0.020967209591\n",
      "33 0.0209652713155\n",
      "34 0.0209372561681\n",
      "35 0.0208828903269\n",
      "36 0.0208587476873\n",
      "37 0.0208273177294\n",
      "38 0.0208158793943\n",
      "39 0.0207795743961\n"
     ]
    }
   ],
   "source": [
    "model = Sentiment()\n",
    "criterion = torch.nn.BCELoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-0)\n",
    "\n",
    "for t in range(40):\n",
    "    \n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    for words, tag in train:\n",
    "        if tag == 2:\n",
    "            continue\n",
    "        if tag < 2:\n",
    "            tag = 0.0\n",
    "        if tag > 2:\n",
    "            tag = 1.0\n",
    "        optimizer.zero_grad()\n",
    "        x = torch.tensor([w for w in words])\n",
    "        y_pred = model(x)\n",
    "        # print y_pred\n",
    "        loss = criterion(y_pred, torch.tensor([tag]))\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print t, train_loss/len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev 0.783635365184\n",
      "Train 0.999132947977\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    using = 0\n",
    "    for words, tag in dev:\n",
    "        if tag == 2:\n",
    "            continue\n",
    "        using += 1\n",
    "        if tag < 2:\n",
    "            tag = 0.0\n",
    "        if tag > 2:\n",
    "            tag = 1.0\n",
    "        x = torch.tensor([w for w in words])\n",
    "        y_pred = model(x)\n",
    "        correct += abs(tag - y_pred.item()) < 0.5\n",
    "\n",
    "    print \"dev\", float(correct)/using\n",
    "    correct = 0\n",
    "    using = 0\n",
    "    for words, tag in train:\n",
    "        if tag == 2:\n",
    "            continue\n",
    "        using += 1\n",
    "        if tag < 2:\n",
    "            tag = 0.0\n",
    "        if tag > 2:\n",
    "            tag = 1.0\n",
    "        x = torch.tensor([w for w in words])\n",
    "        y_pred = model(x)\n",
    "        # print y_pred\n",
    "        correct += abs(tag - y_pred.item()) < 0.5\n",
    "\n",
    "    print \"Train\", float(correct)/using  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4653], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, window_size, embedding_dim, output_channels):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        assert window_size%2 == 1 #odd window sizes please\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_channels = output_channels\n",
    "        \n",
    "        self.embed = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.cnn = torch.nn.Conv1d(embedding_dim, output_channels, window_size, padding = (window_size - 1)/2) \n",
    "        self.linear = torch.nn.Linear(output_channels, 1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embed(inputs).t().view(1,self.embedding_dim, -1)\n",
    "        convoluted = self.cnn(embedded)\n",
    "        pooled = F.relu(F.max_pool1d(convoluted, len(inputs)))\n",
    "        return torch.sigmoid(self.linear(pooled.view(self.output_channels)))\n",
    "    \n",
    "cnn = CNN(5,3,32,16)\n",
    "input = torch.tensor([0,1,2,1,4]) \n",
    "# print input\n",
    "cnn(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.559116127963\n",
      "1 0.546992398338\n",
      "2 0.537920873117\n",
      "3 0.529935765009\n",
      "4 0.522986482761\n",
      "5 0.514049974196\n",
      "6 0.506678677761\n",
      "7 0.496691504355\n",
      "8 0.484541071418\n",
      "9 0.478142225802\n",
      "10 0.47074982075\n",
      "11 0.458209036435\n",
      "12 0.449260221628\n",
      "13 0.438598727762\n",
      "14 0.427408823783\n",
      "15 0.418008087933\n",
      "16 0.409858000395\n",
      "17 0.396625886976\n",
      "18 0.38380466141\n",
      "19 0.371819286575\n",
      "20 0.357757684014\n",
      "21 0.34230660512\n",
      "22 0.333373975069\n",
      "23 0.312854016985\n",
      "24 0.296530620671\n",
      "25 0.282677547839\n",
      "26 0.266663719459\n",
      "27 0.242970353538\n",
      "28 0.228311647407\n",
      "29 0.211016473039\n",
      "30 0.195309464113\n",
      "31 0.1568078539\n",
      "32 0.141335186566\n",
      "33 0.12904788398\n",
      "34 0.107520385369\n",
      "35 0.0945435691704\n",
      "36 0.0705132258136\n",
      "37 0.0588438163109\n",
      "38 0.0405577773309\n",
      "39 0.026601341458\n"
     ]
    }
   ],
   "source": [
    "model = CNN(len(w2i),3,16,16)\n",
    "\n",
    "criterion = torch.nn.BCELoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum = 0.1)\n",
    "\n",
    "for t in range(40):\n",
    "    \n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    for words, tag in train:\n",
    "        if tag == 2:\n",
    "            continue\n",
    "        if tag < 2:\n",
    "            tag = 0.0\n",
    "        if tag > 2:\n",
    "            tag = 1.0\n",
    "        optimizer.zero_grad()\n",
    "        x = torch.tensor([w for w in words])\n",
    "        y_pred = model(x)\n",
    "        # print y_pred\n",
    "        loss = criterion(y_pred, torch.tensor([tag]))\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print t, train_loss/len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev 0.668863261944\n",
      "Train 0.996242774566\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    using = 0\n",
    "    for words, tag in dev:\n",
    "        if tag == 2:\n",
    "            continue\n",
    "        using += 1\n",
    "        if tag < 2:\n",
    "            tag = 0.0\n",
    "        if tag > 2:\n",
    "            tag = 1.0\n",
    "        x = torch.tensor([w for w in words])\n",
    "        y_pred = model(x)\n",
    "        correct += abs(tag - y_pred.item()) < 0.5\n",
    "\n",
    "    print \"dev\", float(correct)/using\n",
    "    correct = 0\n",
    "    using = 0\n",
    "    for words, tag in train:\n",
    "        if tag == 2:\n",
    "            continue\n",
    "        using += 1\n",
    "        if tag < 2:\n",
    "            tag = 0.0\n",
    "        if tag > 2:\n",
    "            tag = 1.0\n",
    "        x = torch.tensor([w for w in words])\n",
    "        y_pred = model(x)\n",
    "        # print y_pred\n",
    "        correct += abs(tag - y_pred.item()) < 0.5\n",
    "\n",
    "    print \"Train\", float(correct)/using  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5408]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embed = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = torch.nn.RNN(embedding_dim, hidden_dim) \n",
    "        self.linear = torch.nn.Linear(hidden_dim, 1)\n",
    "        self.hidden = None\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embed(inputs).view(len(inputs),1,-1)\n",
    "        #print embedded\n",
    "        out, self.hidden = self.rnn(embedded, self.hidden)\n",
    "        return torch.sigmoid(self.linear(out)[-1].view(1,1))\n",
    "    \n",
    "    def initialize(self):\n",
    "        self.hidden = torch.zeros(1,1, self.hidden_dim)\n",
    "        \n",
    "rnn = SimpleRNN(3,5,2)\n",
    "rnn(torch.tensor([1,0,1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-99e295056cbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Dave/anaconda/envs/py3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Dave/anaconda/envs/py3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = SimpleRNN(len(w2i),16,16)\n",
    "\n",
    "\n",
    "criterion = torch.nn.BCELoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum = 0.9)\n",
    "\n",
    "\n",
    "train = train #[:500]\n",
    "for t in range(40):\n",
    "    \n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    for words, tag in train:\n",
    "        if tag == 2:\n",
    "            continue\n",
    "        if tag < 2:\n",
    "            tag = 0.0\n",
    "        if tag > 2:\n",
    "            tag = 1.0\n",
    "        optimizer.zero_grad()\n",
    "        model.initialize()\n",
    "        for w in words:\n",
    "            x = torch.tensor([w])\n",
    "            y_pred = model(x)\n",
    "\n",
    "        loss = criterion(y_pred, torch.tensor([tag]).view(1,1))\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print (t, train_loss/len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5692605257034302\n",
      "1 0.5648662068843842\n",
      "2 0.5603104606866837\n",
      "3 0.5570976375937462\n",
      "4 0.5519396099448204\n",
      "5 0.5455788746476173\n",
      "6 0.540050355732441\n",
      "7 0.5390247922837734\n",
      "8 0.5301990541517735\n",
      "9 0.5262349545657635\n",
      "10 0.5202215259671211\n",
      "11 0.5126709409654141\n",
      "12 0.5063853273987771\n",
      "13 0.5022973581552506\n",
      "14 0.493425964474678\n",
      "15 0.4844797843694687\n",
      "16 0.4785664509385824\n",
      "17 0.4577317737787962\n",
      "18 0.4482614317238331\n",
      "19 0.43179953045397995\n",
      "20 0.40643735373392703\n",
      "21 0.3951086123175919\n",
      "22 0.36012182890996336\n",
      "23 0.3329659046009183\n",
      "24 0.30500614741444587\n",
      "25 0.2707595526473597\n",
      "26 0.24816703260596842\n",
      "27 0.21749827189650386\n",
      "28 0.2169035478234291\n",
      "29 0.1641898257934954\n",
      "30 0.13485574983432888\n",
      "31 0.11247693310817704\n",
      "32 0.09395424605731387\n",
      "33 0.1744057827568613\n",
      "34 0.19947989458573284\n",
      "35 0.10215563367280993\n",
      "36 0.054995651553676\n",
      "37 0.03323686621818342\n",
      "38 0.022800288593192816\n",
      "39 0.016619373231915233\n"
     ]
    }
   ],
   "source": [
    "model = SimpleRNN(len(w2i),16,16)\n",
    "\n",
    "\n",
    "criterion = torch.nn.BCELoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum = 0.9)\n",
    "\n",
    "\n",
    "train = train[:500]\n",
    "for t in range(40):\n",
    "    \n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    for words, tag in train:\n",
    "        if tag == 2:\n",
    "            continue\n",
    "        if tag < 2:\n",
    "            tag = 0.0\n",
    "        if tag > 2:\n",
    "            tag = 1.0\n",
    "        optimizer.zero_grad()\n",
    "        model.initialize()\n",
    "\n",
    "        x = torch.tensor([w for w in words])\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = criterion(y_pred, torch.tensor([tag]).view(1,1))\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print (t, train_loss/len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5921]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BiRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(BiRNN, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embed = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = torch.nn.RNN(embedding_dim, hidden_dim, bidirectional=True) \n",
    "        self.linear = torch.nn.Linear(2*hidden_dim, 1)\n",
    "        self.hidden = None\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embed(inputs).view(len(inputs),1,-1)\n",
    "        out, self.hidden = self.rnn(embedded, self.hidden)\n",
    "        out = torch.cat((out[-1][0][0:self.hidden_dim], out[0][0][self.hidden_dim:])).view(1,-1)\n",
    "        return torch.sigmoid(self.linear(out))\n",
    "    \n",
    "    def initialize(self):\n",
    "        self.hidden = torch.zeros(2,1, self.hidden_dim)\n",
    "        \n",
    "rnn = BiRNN(3,1,2)\n",
    "rnn(torch.tensor([1,0,1,0,1,0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0333554408886\n",
      "1 0.0330993935801\n",
      "2 0.0329459922123\n",
      "3 0.0328314971276\n",
      "4 0.0327541858118\n",
      "5 0.032677121\n",
      "6 0.0326076899812\n",
      "7 0.0325467582576\n",
      "8 0.0324889212247\n",
      "9 0.0324354128535\n",
      "10 0.0323851123724\n",
      "11 0.0323335546093\n",
      "12 0.0322865425131\n",
      "13 0.0322400705701\n",
      "14 0.0321906176657\n",
      "15 0.032143055859\n",
      "16 0.0321025188684\n",
      "17 0.0320543658567\n",
      "18 0.032011705067\n",
      "19 0.0319664685146\n",
      "20 0.03191965997\n",
      "21 0.0318717082286\n",
      "22 0.0318258629121\n",
      "23 0.0317757621038\n",
      "24 0.0317296407112\n",
      "25 0.0316744882523\n",
      "26 0.0316320694146\n",
      "27 0.031581087596\n",
      "28 0.0315288390061\n",
      "29 0.0314772245213\n",
      "30 0.0314072049886\n",
      "31 0.0313700506976\n",
      "32 0.0313115433636\n",
      "33 0.0312559163836\n",
      "34 0.0311904747339\n",
      "35 0.0311333001016\n",
      "36 0.0310749847543\n",
      "37 0.0310104609945\n",
      "38 0.0309397878303\n",
      "39 0.0308863904732\n"
     ]
    }
   ],
   "source": [
    "model = BiRNN(len(w2i),16,16)\n",
    "\n",
    "\n",
    "criterion = torch.nn.BCELoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum = 0.1)\n",
    "\n",
    "\n",
    "train_short = train[:500]\n",
    "for t in range(40):\n",
    "    \n",
    "    random.shuffle(train_short)\n",
    "    train_loss = 0.0\n",
    "    for words, tag in train_short:\n",
    "        if tag == 2:\n",
    "            continue\n",
    "        if tag < 2:\n",
    "            tag = 0.0\n",
    "        if tag > 2:\n",
    "            tag = 1.0\n",
    "        optimizer.zero_grad()\n",
    "        model.initialize()\n",
    "\n",
    "        x = torch.tensor([w for w in words])\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = criterion(y_pred, torch.tensor([tag]))\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print t, train_loss/len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4904]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BiLSTM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embed = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = torch.nn.LSTM(embedding_dim, hidden_dim, bidirectional=True) \n",
    "        self.linear = torch.nn.Linear(2*hidden_dim, 1)\n",
    "        self.hidden = None\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embed(inputs).view(len(inputs),1,-1)\n",
    "        out, self.hidden = self.rnn(embedded, self.hidden)\n",
    "        out = torch.cat((out[-1][0][0:self.hidden_dim], out[0][0][self.hidden_dim:])).view(1,-1)\n",
    "        return torch.sigmoid(self.linear(out))\n",
    "    \n",
    "    def initialize(self):\n",
    "        self.hidden = (torch.zeros(2,1, self.hidden_dim), torch.zeros(2,1, self.hidden_dim))\n",
    "        \n",
    "rnn = BiLSTM(3,1,2)\n",
    "rnn(torch.tensor([1,0,1,0,1,0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.564201382371\n",
      "1 0.550223903024\n",
      "2 0.526209019758\n",
      "3 0.498078458186\n",
      "4 0.460739228106\n",
      "5 0.416743377978\n",
      "6 0.367283961047\n",
      "7 0.312497644719\n",
      "8 0.257001315494\n",
      "9 0.206403194881\n",
      "10 0.164677421874\n",
      "11 0.118634913678\n",
      "12 0.105030022414\n",
      "13 0.0839344270636\n",
      "14 0.0520474091161\n",
      "15 0.0322553226341\n",
      "16 0.0174046786955\n",
      "17 0.0456262493326\n",
      "18 0.0777544113805\n",
      "19 0.0908013209474\n",
      "20 0.0625696961775\n",
      "21 0.0277768347734\n",
      "22 0.0122808843571\n",
      "23 0.00462367299147\n",
      "24 0.00157571853174\n",
      "25 0.000983569798029\n",
      "26 0.000766159769663\n",
      "27 0.000631276969493\n",
      "28 0.000539195835022\n",
      "29 0.000471172976379\n",
      "30 0.000418577219913\n",
      "31 0.000376040047462\n",
      "32 0.000341003955958\n",
      "33 0.000311871213067\n",
      "34 0.000287183237383\n",
      "35 0.000266152927249\n",
      "36 0.000247947381376\n",
      "37 0.000231780313911\n",
      "38 0.000217635858367\n",
      "39 0.00020525988874\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM(len(w2i),16,16)\n",
    "\n",
    "\n",
    "criterion = torch.nn.BCELoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum = 0.9)\n",
    "\n",
    "\n",
    "train_short = train #[:500]\n",
    "for t in range(40):\n",
    "    \n",
    "    random.shuffle(train_short)\n",
    "    train_loss = 0.0\n",
    "    for words, tag in train_short:\n",
    "        if tag == 2:\n",
    "            continue\n",
    "        if tag < 2:\n",
    "            tag = 0.0\n",
    "        if tag > 2:\n",
    "            tag = 1.0\n",
    "        optimizer.zero_grad()\n",
    "        model.initialize()\n",
    "\n",
    "        x = torch.tensor([w for w in words])\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = criterion(y_pred, torch.tensor([tag]))\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print t, train_loss/len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
